{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "municipal-neighbor",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "partial-photograph",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 2, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 2, 0]\n",
      "f [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.99\n",
    "\n",
    "def policy_evaluation(env, pi, gamma=0.99, eps=1e-5):\n",
    "    n_states = env.observation_space.n\n",
    "    \n",
    "    V = np.ones(n_states)\n",
    "    V[n_states-1] = 0\n",
    "    delta = 1\n",
    "    \n",
    "    while delta > eps:\n",
    "        delta = 0\n",
    "        for s in range(n_states):\n",
    "            v = V[s]\n",
    "            a = pi[s]\n",
    "            \n",
    "            v_sum = 0\n",
    "            for next_s in range(n_states):\n",
    "#                 print(s, next_s, P[next_s, s])\n",
    "                v_sum += P[next_s, s, a]*(0 + gamma*V[next_s])  \n",
    "            \n",
    "            V[s] = v_sum\n",
    "#             print(s, V[s])\n",
    "            delta = max(delta, abs(v - V[s]))\n",
    "#         break\n",
    "            \n",
    "    return V\n",
    "    \n",
    "def policy_improvement(env, pi, V):\n",
    "    n_states = env.observation_space.n\n",
    "    n_actions = env.action_space.n\n",
    "    \n",
    "    stable = True\n",
    "    \n",
    "    for s in range(n_states):\n",
    "        a_old = pi[s]\n",
    "        \n",
    "        action_values = np.zeros(n_actions)\n",
    "        for a in range(n_actions):\n",
    "            \n",
    "            action_value = 0\n",
    "            for next_s in range(n_states):\n",
    "                action_value += P[next_s, s, a]*(0 + gamma*V[next_s])\n",
    "            \n",
    "            action_values[a] = action_value\n",
    "            \n",
    "        pi[s] = np.argmax(action_values)\n",
    "        if pi[s] != a_old:\n",
    "            stable = False\n",
    "            \n",
    "    return pi, stable\n",
    "        \n",
    "        \n",
    "env = gym.make('FrozenLake-v0', is_slippery=False)\n",
    "\n",
    "# print(pi)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_valid_actions(s1, s2):\n",
    "    \n",
    "    valid_actions = np.zeros(n_actions)\n",
    "    \n",
    "    #left\n",
    "    if s1-s2==1 and s1%4 > 0:\n",
    "        valid_actions[0] = 1\n",
    "    \n",
    "    #down\n",
    "    if s1-s2==-4:\n",
    "        valid_actions[1] = 1\n",
    "    \n",
    "    # right\n",
    "    if s1-s2==-1 and s2%4 < 3:\n",
    "        valid_actions[2] = 1\n",
    "        \n",
    "    # up\n",
    "    if s1-s2==4:\n",
    "        valid_actions[3] = 1\n",
    "        \n",
    "#     if s1==s2:\n",
    "#         # left\n",
    "#         # down\n",
    "#         # right\n",
    "#         if s1%4==3:\n",
    "#             valid_actions[3] = 1\n",
    "#         # up\n",
    "#         if s1<4:\n",
    "#             valid_actions[3] = 1\n",
    "        \n",
    "    return valid_actions\n",
    "\n",
    "n_actions = env.action_space.n\n",
    "n_states = env.observation_space.n\n",
    "P = np.zeros(shape=(n_states, n_states, n_actions))\n",
    "\n",
    "for s1 in range(n_states):\n",
    "    for s2 in range(n_states):\n",
    "        P[s2, s1] = get_valid_actions(s1, s2)\n",
    "\n",
    "def policy_iteration(env):\n",
    "\n",
    "    pi = np.random.randint(n_actions, size=n_states)\n",
    "    pi = [2,2,1,0,0,0,1,0,0,0,1,0,0,0,2,0]\n",
    "    print(pi)\n",
    "    stable = False\n",
    "    \n",
    "    while not stable: \n",
    "        V = policy_evaluation(env, pi)\n",
    "        print('V')\n",
    "        pi, stable = policy_improvement(env, pi, V)\n",
    "        print(pi)\n",
    "        break\n",
    "        \n",
    "policy_iteration(env)\n",
    "# P[s',s,a] = P(s'|s,a)\n",
    "# Note that the transition matrix is not needed, since transitions are deterministic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "level-integral",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pi(state):\n",
    "    \n",
    "    return np.random.randint(4)\n",
    "\n",
    "def evaluate_policy_TD(env, pi, num_episodes, alpha, gamma):\n",
    "    \n",
    "    V = np.zeros(env.observation_space.n)\n",
    "    \n",
    "\n",
    "    for i in range(num_episodes):\n",
    "    \n",
    "        done = False\n",
    "        state = env.reset()\n",
    "        \n",
    "        while not done:\n",
    "\n",
    "            action = pi(state)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            V[state] += alpha*(reward + gamma*V[next_state] - V[state])\n",
    "            state = next_state\n",
    "            \n",
    "    return V\n",
    "\n",
    "\n",
    "def evaluate_policy_MC(env, pi, num_episodes, alpha, gamma):\n",
    "    \n",
    "    V = np.zeros(env.observation_space.n)\n",
    "    states  = []\n",
    "    rewards = []\n",
    "\n",
    "    for i in range(num_episodes):\n",
    "    \n",
    "        done = False\n",
    "        rewards.clear()\n",
    "        state = env.reset()\n",
    "        states.append(state)\n",
    "\n",
    "        while not done:\n",
    "            \n",
    "            action = pi(state)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            \n",
    "            states.append(next_state)\n",
    "            rewards.append(reward)\n",
    "            \n",
    "            state = next_state\n",
    "            \n",
    "        reward_sum = 0\n",
    "        \n",
    "        for state in states:\n",
    "            for i in range(len(rewards)):\n",
    "                reward_sum += gamma**i * rewards[i]\n",
    "\n",
    "            V[state] = reward_sum\n",
    "            \n",
    "\n",
    "    return V\n",
    "        \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "seven-metabolism",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v0')\n",
    "evaluate_policy_MC(env, pi, 1000, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "isolated-lesbian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive implementation (for loops are slow), but matches the box\n",
    "def policy_iter(env, gamma, theta):\n",
    "    \"\"\"Policy Iteration Algorithm\n",
    "    \n",
    "    Params:\n",
    "        env - environment with following required memebers:\n",
    "            env.nb_states - number of states\n",
    "            env.nb_action - number of actions\n",
    "            env.model     - prob-transitions and rewards for all states and actions, see note #1\n",
    "        gamma (float) - discount factor\n",
    "        theta (float) - termination condition\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Initialization\n",
    "    V = np.ones(env.nb_states)\n",
    "    pi = np.zeros(env.nb_states, dtype=int)  # greedy, always pick action 0\n",
    "    \n",
    "    while True:\n",
    "    \n",
    "        # 2. Policy Evaluation\n",
    "        while True:\n",
    "            delta = 0\n",
    "            for s in range(env.nb_states):\n",
    "                v = V[s]\n",
    "                V[s] = sum_sr(env, V=V, s=s, a=pi[s], gamma=gamma)\n",
    "                delta = max(delta, abs(v - V[s]))\n",
    "            if delta < theta: break\n",
    "\n",
    "        # 3. Policy Improvement\n",
    "        policy_stable = True\n",
    "        for s in range(env.nb_states):\n",
    "            old_action = pi[s]\n",
    "            pi[s] = np.argmax([sum_sr(env, V=V, s=s, a=a, gamma=gamma)  # list comprehension\n",
    "                               for a in range(env.nb_actions)])\n",
    "            if old_action != pi[s]: policy_stable = False\n",
    "        if policy_stable: break\n",
    "    \n",
    "    return V, pi\n",
    "\n",
    "def sum_sr(env, V, s, a, gamma):\n",
    "    \"\"\"Calc state-action value for state 's' and action 'a'\"\"\"\n",
    "    tmp = 0  # state value for state s\n",
    "    for p, s_, r, _ in env.model[s][a]:     # see note #1 !\n",
    "        # p  - transition probability from (s,a) to (s')\n",
    "        # s_ - next state (s')\n",
    "        # r  - reward on transition from (s,a) to (s')\n",
    "        tmp += p * (r + gamma * V[s_])\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "liberal-consent",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "[[0.82352925 0.82352919 0.82352915 0.82352913]\n",
      " [0.82352926 0.         0.52941165 0.        ]\n",
      " [0.82352929 0.82352932 0.7647058  0.        ]\n",
      " [0.         0.88235288 0.94117644 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v0')\n",
    "env.reset()\n",
    "env.render()\n",
    "\n",
    "if not hasattr(env, 'nb_states'):  env.nb_states = env.env.nS\n",
    "if not hasattr(env, 'nb_actions'): env.nb_actions = env.env.nA\n",
    "if not hasattr(env, 'model'):      env.model = env.env.P\n",
    "\n",
    "V, pi = policy_iter(env, gamma=1.0, theta=1e-8)\n",
    "print(V.reshape([4, -1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decreased-constitutional",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
